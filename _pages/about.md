---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<style type="text/css">
#touch {
 background-color: #bbb;
 padding: .4em;
 -moz-border-radius: 5px;
 -webkit-border-radius: 5px;
 border-radius: 6px;
 color: #fff;
 font-size: 14px;
 text-decoration: none;
 border: none;
}
#touch:hover {
 border: none;
 background: orange;
 box-shadow: 0px 0px 1px #777;
}
</style>

<style type="text/css">
a:link {text-decoration: none; }
a:hover { text-decoration: underline; }
</style>

------

I am a PhD student in the <a target="_blank" href="https://cecs.anu.edu.au/">College of Engineering and Computer Science (CECS)<a/> at the <a target="_blank" href="https://www.anu.edu.au/">Australian National University (ANU)<a/>. I am also a research student at the <a target="_blank" href="https://www.roboticvision.org/">Australian Centre for Robotic Vision (ACRV)<a/> and the <a target="_blank" href="https://v3alab.github.io/#about">Vision-Ask-Answer-Act Lab (V3A)<a/>.

I am under the supervision of <a target="_blank" href="https://users.cecs.anu.edu.au/~Robert.Mahony/">Prof. Robert Mahony<a/> (ANU), <a target="_blank" href="https://scholar.google.com/citations?user=DeHsa3wAAAAJ&hl=en">Dr. Yonhon Ng<a/> (ANU) and <a target="_blank" href="https://users.cecs.anu.edu.au/~nmb/">Prof. Nick Barnes<a/> (ANU).

Prior to that, in Nov'2018, I received my bachelor degree of engineering in mechatronic systems with a first-class honours in the College of Engineering and Computer Science at ANU. In 2018, I was also a part-time research student at the <a target="_blank" href="https://data61.csiro.au/">Data61, CSIRO<a/>, working on human pose and shape visualization.

I have a broad research interests in computer vision, natural language processing and robotics. Currently, my main research focus is on the problem of **Embodied Vision-and-Language (E-V&L)**, and inparticular Vision-and-Language Navigation . I believe, E-V&L has the great potential to merge a wide range of visiolinguistic research to create an embodied interative system which is practical to assist human in real-world.

------

News
======

**2022.06.19**
- Attending CVPR2022 in person!!! Finally meeting so many great researchers! I have learned so much!!! ❤️❤️❤️
- Congrats to Zun Wang, Dong An and Team **JoyBoy** for winning the **1st Place in the Room-Across-Room (RxR) Habitat Challenge 2022**!!! 😆⚡⚡ <a target="_blank" href="https://arxiv.org/abs/2206.11610"><button id="touch">Report</button></a> <a target="_blank" href="https://drive.google.com/file/d/15VbXcanw7D3q5TUm75WmDVslqgOmVvJk/view?usp=sharing"><button id="touch">Certificate</button></a>

**2022.06.15** &emsp; Visiting Professor Eric Xin Wang and the **ERIC Lab at the University of California Santa Cruz**! It was amazing to learn from so many young researchers! 😄
  
**2022.05.10** &emsp; Invited talk by the **NLP Lab at the Fudan University**, really enjoyed chatting with everyone! 😄
  
**2022.03.14** &emsp; I have started a research internship at the **Creative Intelligence Lab in Adobe Research** in San Jose, California, US!!! 😆😆😆
  
**2022.03.02**
- Our paper **Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation** has been accepted to CVPR 2022! 😊 I am so happy to share lots of thoughts about VLN in this paper! See you guys in New Orleans! ❤️ <a target="_blank" href="https://arxiv.org/abs/2203.02764"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Discrete-Continuous-VLN"><button id="touch">Code</button></a>
- Paper **HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation** by Yanyuan Qiao, Yuankai Qi, Peng Wang, Qi Wu and myself has been accepted to CVPR 2022! Congrats Yanyuan on the first paper in her PhD! 😀 <a target="_blank" href="https://arxiv.org/abs/2203.11591"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YanyuanQiao/HOP-VLN"><button id="touch">Code</button></a>
  
**2021.08.17** &emsp; Paper **The Road To Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation** by Yuankai Qi, Zizheng Pan, Ming-Hsuan Yang, Anton van den Hengel, Qi Wu and myself has been accepted to ICCV 2021! 😀 <a target="_blank" href="https://arxiv.org/abs/2104.04167"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YuankaiQi/ORIST"><button id="touch">Code</button></a>

**2021.04.10** &emsp; Paper **Learning Structure-Aware Semantic Segmentation with Image-Level Supervision** by Jiawei Liu, Dr. Jing Zhang, Prof. Nick Barnes and myself, has been accepted to IJCNN 2021! Congrats Jiawei on his first paper in computer vision! 😀 <a target="_blank" href="https://arxiv.org/abs/2104.07216"><button id="touch">PDF</button></a>

**2021.03.16** &emsp; Our <a target="_blank" href="https://github.com/YicongHong/Thinking-VLN">Thinking-VLN</a> repo is online! Come to enjoy our immature ideas and share your thoughts! Just for FUN thinking!

**2021.03.06** &emsp; Our paper **A Recurrent Vision-and-Language BERT for Navigation** has been accepted to CVPR 2021 as an Oral paper with 3 strong accepts! 😆😆😆 <a target="_blank" href="https://arxiv.org/abs/2011.13922"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Recurrent-VLN-BERT"><button id="touch">Code</button></a>

**2020.10.05** &emsp; I gave a guest lecture in the Deep Learning Course at ANU (ENGN8536) about Vision and Language Research! My first lecture at Uni! Nervous and Fun! 😀 <a target="_blank" href="https://drive.google.com/file/d/1Rsy8gFK0seWVgDJ6Uc0UU9MXO9F23EKY/view?usp=sharing"><button id="touch">PDF</button></a>

**2020.09.26** &emsp; Our paper **Language and Visual Entity Relationship Graph for Agent Navigation** has been accepted to NeurIPS 2020! 😀 <a target="_blank" href="https://arxiv.org/abs/2010.09304"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Entity-Graph-VLN"><button id="touch">Code</button></a>

**2020.09.15** &emsp; Our paper **Sub-Instruction Aware Vision-and-Language Navigation** has been accepted to EMNLP 2020! My first paper! 😊 <a target="_blank" href="https://arxiv.org/abs/2004.02707"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Fine-Grained-R2R"><button id="touch">FGR2R Data</button></a>

------

Research
======
<a target="_blank" href="https://arxiv.org/abs/2203.02764">**Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation**<a/><br>
**Yicong Hong**, Zun Wang, Qi Wu, Stephen Gould<br>
<em>Conference on Computer Vision and Pattern Recognition (CVPR), 2022<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2203.02764"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Discrete-Continuous-VLN"><button id="touch">Code</button></a>
  
<a target="_blank" href="https://arxiv.org/abs/2011.13922">**A Recurrent Vision-and-Language BERT for Navigation**<a/><br>
**Yicong Hong**, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould<br>
<em>Conference on Computer Vision and Pattern Recognition (CVPR), 2021<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2011.13922"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Recurrent-VLN-BERT"><button id="touch">Code</button></a>

<a target="_blank" href="https://arxiv.org/abs/2010.09304">**Language and Visual Entity Relationship Graph for Agent Navigation**<a/><br>
**Yicong Hong**, Cristian Rodriguez-Opazo, Yuankai Qi, Qi Wu, Stephen Gould<br>
<em>Conference on Neural Information Processing Systems (NeurIPS), 2020<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2010.09304"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Entity-Graph-VLN"><button id="touch">Code</button></a>

<a target="_blank" href="https://arxiv.org/abs/2004.02707">**Sub-Instruction Aware Vision-and-Language Navigation**<a/><br>
**Yicong Hong**, Cristian Rodriguez-Opazo, Qi Wu, Stephen Gould<br>
<em>Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2004.02707"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YicongHong/Fine-Grained-R2R"><button id="touch">FGR2R Data</button></a>

<a target="_blank" href="https://arxiv.org/abs/2104.04167">**HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation**<a/><br>
Yanyuan Qiao, Yuankai Qi, **Yicong Hong**, Zheng Yu, Peng Wang, Qi Wu<br>
<em>Conference on Computer Vision and Pattern Recognition (CVPR), 2022<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2203.11591"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YanyuanQiao/HOP-VLN"><button id="touch">Code</button></a>
  
<a target="_blank" href="https://arxiv.org/abs/2104.04167">**The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation**<a/><br>
Yuankai Qi, Zizheng Pan, **Yicong Hong**, Ming-Hsuan Yang, Anton van den Hengel, Qi Wu<br>
<em>International Conference on Computer Vision Systems (ICCV), 2021<em/><br>
<a target="_blank" href="https://arxiv.org/abs/2104.04167"><button id="touch">PDF</button></a> <a target="_blank" href="https://github.com/YuankaiQi/ORIST"><button id="touch">Code</button></a>

